## ABSTRACT
可编程分组调度使得调度算法能够在无需重新设计硬件的情况下集成到交换机中。Push-In First-Out（PIFO）队列为可编程分组调度器提供了一种机制，能够灵活地支持单一调度算法。然而，多租户数据中心（MTDC）中所需的层次化调度仍然是不可编程的。动态且多样化的层次化调度算法往往需要改变 PIFO 队列的数量及其连接拓扑结构，这在固定硬件上支持这些算法带来了巨大挑战。

在本文中，我们提出了虚拟化 PIFO（vPIFO）系统，这是一种面向多租户数据中心中可编程层次化分组调度的硬件虚拟化解决方案。vPIFO 系统仅依赖一个物理 PIFO，即可灵活构建具有多种形态的 PIFO 树，从而使网络运营者能够定制细粒度的流量调度策略。我们给出了一个高性能、可大规模扩展的 vPIFO 硬件设计，并在 FPGA 和 ASIC 上实现了原型系统。在 GlobalFoundries 28nm 工艺下的综合结果表明，vPIFO 在支持最多 128 个 PIFO 实例、6 层层次化调度的情况下，仍可实现高达 400 Gbps 的吞吐率。

据我们所知，vPIFO 是首个通过硬件虚拟化方式支持**精确可编程层次化分组调度**的分组调度器方案。
## INTRODUCTION
包括 Google Cloud、Microsoft Azure 和 Amazon Web Services（AWS）在内的现代云服务提供商，通常采用**多租户数据中心（MultiTenant Data Center，MTDC）**基础设施来向客户交付云计算服务 [10, 26]。在 MTDC 中，多个租户（即用户或组织）共享网络资源。为了履行服务等级协议（Service Level Agreement，SLA），网络运营者必须确保租户之间的隔离性以及资源的确定性保障。此外，租户往往同时运行大量具有不同性能需求的应用。为了实现细粒度的服务质量（Quality of Service，QoS），分配给租户的资源需要在这些应用之间进行合理分配，以满足其各自的需求。

**分组调度**是网络管理中的关键组成部分，用于保障服务质量。调度算法通常针对特定的调度目标进行设计。例如，加权公平队列（Weighted Fair Queueing，WFQ）[12] 用于保证公平的资源分配；pFabric [4] 用于最小化对时延敏感应用的流完成时间；而严格优先级（Strict Priority，SP）则用于区分不同优先级的流量。

同时满足**租户级**和**应用级**的需求，必须依赖**层次化调度算法**。在 MTDC 中，不同企业的调度需求差异很大。例如，门户网站可能将互联网访问流量和在线游戏流量视为关键业务，而金融行业则更关注实时交易流量。此外，企业租户往往会将网络资源进一步分配给内部部门和项目，从而形成二级、三级租户结构，加深调度层次 [22, 34, 41]。当部门结构或项目发生变化时，调度需求也会随之动态调整。为了适应多样且不断变化的调度需求，MTDC 需要具备支持**可编程层次化调度**的调度器。

基于软件的层次化调度器具有良好的可编程性，但在高速网络环境中难以达到线速转发 [32]。在工业界，包括 Broadcom、Cisco、Juniper 和 Huawei 在内的多家厂商，已经在商用交换机中实现了层次化调度策略 [9, 11, 16, 19]。然而，这些交换机通常只能支持固定且较为简单的调度算法，难以灵活适配 MTDC 中动态变化的调度需求。要在**线速条件下支持任意层次化调度任务**，亟需一种硬件可编程的层次化调度器。

Push-In-First-Out（PIFO）[33] 是一种广受关注的可编程调度模型。一个 PIFO 队列与一个**秩（rank）计算函数**相关联，该函数通过分组事务（Packet Transaction）为每个分组分配一个秩值，并根据秩值维护一个优先级队列对分组进行缓存。通过改变秩计算函数，PIFO 队列可以实现多种调度算法。例如，使用虚拟完成时间作为秩可以实现 WFQ，而使用优先级等级作为秩则可以实现 SP [33]。通过将多个 PIFO 连接成一棵 **PIFO 树（PIFO Tree）**，并按层级对分组进行调度，即可实现层次化调度。然而，仅仅将硬件 PIFO 进行静态连接，并不足以支持真正的可编程层次化调度。当调度需求发生变化时，必须重构 PIFO 树，而这在固定硬件上是极具挑战性的。

为了解决硬件中可编程层次化调度的问题，我们提出了一种名为 **vPIFO（virtualized PIFO）** 的虚拟化调度系统。该系统对一个物理 PIFO 调度器进行虚拟化，使其能够在硬件资源约束下实例化任意形态的逻辑 PIFO 树。vPIFO 可以在**无需任何硬件修改**的情况下，适应不断变化的调度目标。它支持基于应用、用户、虚拟局域网（VLAN）、网络切片（slice）以及物理端口等多种维度的流量管理。vPIFO 的一个实际应用场景是在高速交换设备中，用于同时对多个低速物理端口的流量进行调度。

除 MTDC 外，vPIFO 还适用于多种由不同实体共享的物理网络基础设施，例如：由住宅用户和企业用户共同使用的 ISP 汇聚网络、连接总部、分支机构和远程办公场所的私有广域网（WAN），以及为多个切片服务的 5G 无线接入网（RAN）。

本文的主要贡献总结如下：

- **我们设计了 vPIFO 调度系统**，首次将虚拟化思想引入 PIFO。vPIFO 是首个虚拟化的可编程分组调度器，能够在调度实例数量、层级结构以及调度算法等方面提供高度灵活性，从而精确实现层次化调度策略，并提供细粒度的 QoS 保障。  
    在 vPIFO 系统中，我们提出了**调度描述语言（Scheduling Description Language）及其编译器**，使网络运营者和租户能够方便地描述调度需求。同时，我们设计了一种**高性价比、支持虚拟化的 PIFO 引擎（PIFO Engine）**，具有高工作频率和大规模能力。此外，我们引入 **PIFO Visor** 来进行资源管理与分配，从而优化 PIFO 引擎的资源利用率。
    
- **我们在 XCU200 FPGA 上使用 SystemVerilog 实现了 vPIFO 的硬件设计**。相比原始 PIFO 实现 [33] 在 XCU200 FPGA 上只能支持一个、容量为 4096 流的逻辑 PIFO，vPIFO 最多可支持 24 个逻辑 PIFO，每个逻辑 PIFO 的容量可达 5460 条流。在该规模下，vPIFO 可在三级层次化调度算法中，以 100 Gbps 的速率调度 MTU 大小的分组。与最先进的可编程调度器 BMW-Tree [40] 相比，vPIFO 在提供虚拟化支持的同时，仅增加不到 0.4% 的硬件资源消耗，并保持了相近的工作频率。总体而言，vPIFO 在实现高吞吐和大规模的同时，支持精确的可编程层次化调度。
    
- **我们在 28nm ASIC 工艺下对 vPIFO 的 SystemVerilog 代码进行了综合**。vPIFO 可以在最多 128 个逻辑 PIFO 的规模下灵活支持层次化调度，而不受层级深度或各层逻辑 PIFO 数量的限制。在 6 层调度层次结构下，vPIFO 可实现高达 400 Gbps 的吞吐率。该项目已开源，地址为：https://github.com/vPIFOimage/vPIFO。
## BACKGROUND AND MOTIVATION
### 层次化调度算法（Hierarchical Scheduling Algorithm）

调度算法在保障服务质量方面起着至关重要的作用。传统的 QoS DiffServ 模型采用**扁平化调度（flat scheduling）**，无法对来自不同租户的多种流量类型进行有效区分 [41]。为了同时满足**租户级**和**应用级**的性能需求，分组调度必须采用**层次化调度策略**。

图 1 展示了一个 MTDC 中层次化调度策略的简单示例。在该策略树中，顶部的“叶子节点”（A、B、C、D、E）表示应用，它们是分组的来源；非叶子节点（Root、X、Y、Z）表示用于管理流量的调度器。流量在**跨租户组（inter-tenant-group）**、**租户组内（intra-tenant-group）**以及**应用级**三个层面接受调度管理：租户被划分为 VIP 组和普通组，根节点 Root 规定 VIP 租户的流量应当严格优先于普通租户的流量；节点 X 和 Y 规定同一租户组内的竞争租户采用 WFQ 算法公平共享带宽；租户 1 包含两个应用，节点 Z 指定在该租户内，Memcached 流量（A）应当严格优先于 Spark 流量（B）。

数据中心运营者面临着**动态且多样化的租户与应用需求**，因此必须部署不同的层次化调度策略。我们将**可编程层次化调度**定义为：在具备足够存储资源的前提下，系统能够在**不修改硬件设计**的情况下，实现多种不同的层次化调度策略¹。为了使网络运营者和租户能够根据自身需求定制调度算法，我们进一步探讨如何在系统中支持可编程的层次化调度。

### PIFO 树（PIFO Tree）

PIFO 是一种广泛使用的可编程调度模型。它引入了**调度树（PIFO Tree）**这一抽象 [33]，使得通过可编程调度节点实现层次化调度成为可能。

一棵 PIFO 树由多个 PIFO 节点组成，每个节点可以被赋予不同的**秩（rank）计算函数**，以实现不同的调度需求。除根节点外，每个 PIFO 节点都包含一个**引用（reference）**，表示该节点所聚合的流量。该引用随后由父节点进行调度，以满足更高层级的调度需求。通过逐层执行 PIFO 调度，流量的发送顺序能够同时满足各个层级的调度要求。

图 2(a) 展示了实现图 1 中层次化调度策略的 PIFO 树。策略树中的每个非叶子节点（调度器）在 PIFO 树中均对应一个 PIFO 节点。当一个分组到达时（即 **Packet In Event**），它会被压入负责该分组调度的最低层 PIFO 节点，即图 1 中的应用级调度器。随后，该 PIFO 节点会将自身的引用压入其父节点，这一过程会递归进行，直至根 PIFO 节点。引用在图中以黑色表示。图 2(b) 展示了来自应用 A 的分组在 Packet In 事件中，各层级的 Push 操作。分组及其引用在 PIFO 节点中的插入位置，由对应节点所采用的调度算法决定。

当链路空闲时，会触发 **Packet Out Event**。此时，根 PIFO 节点弹出队首元素：若该元素是一个分组，则直接转发；若该元素是某个子 PIFO 节点的引用，则向该子节点发送 Pop 请求，并递归执行该过程，直到最终弹出一个分组。图 2(c) 说明了在 Packet Out 事件中，通过一系列 Pop 操作来确定下一个待发送分组的过程：根节点首先弹出一个指向 PIFO-X 的引用，随后 PIFO-X 弹出其队首元素，该元素是来自应用 D 的分组，事件处理至此完成。

实现某一层次化调度策略，需要构建一棵专用的 PIFO 树。而支持可编程层次化调度，意味着需要在**固定硬件**上构造**不同形态的 PIFO 树**：PIFO 节点数量随流量类别数量增长，而 PIFO 树的深度则随着流量管理粒度的细化而增加。尽管单个 PIFO 节点本身是可编程的，但实现真正的可编程层次化调度依然极具挑战性。

### PIFO 树的实现

#### 基于多个 PIFO 队列的实现

Sivaraman 等人 [33] 提出了一种使用多个**全互连物理 PIFO 队列**（基于移位寄存器的优先级队列）的方案，其中，PIFO 树中同一层级的 PIFO 节点共享一个物理 PIFO 队列。具体而言，每个 PIFO 节点都具有一个 ID。当分组进入物理 PIFO 队列时，仅根据其 rank 进行排序，而不考虑 PIFO 节点 ID；在出队时，系统首先在物理队列中筛选属于特定 ID 的分组，然后弹出其中 rank 最小的分组。

然而，该方案存在以下缺陷：  
首先，不同 PIFO 节点之间缺乏隔离机制，单个 PIFO 节点可能独占整个物理队列，从而无法保障所有调度需求；  
其次，PIFO 树的深度受限于物理 PIFO 队列的数量，限制了可编程层次化调度能力；  
最后，基于移位寄存器的实现天然只能支持约 4K 个排序元素，使得所有 PIFO 节点能够调度的流总数被限制在 4K 以内。

鉴于这些限制，简单地连接多个 PIFO 队列，并不适合在 MTDC 中对大量流量进行灵活的层次化调度。虽然可以通过预先实现大量物理 PIFO 队列来满足复杂的层次化调度需求，但其硬件成本过高，而且在部署简单调度算法时会造成计算资源的浪费²。

这引出了一个关键问题：**能否在固定硬件上实现可编程层次化调度，并同时高效利用所有计算资源？**

#### 基于虚拟化的单 PIFO 队列实现

答案是肯定的。我们认为 **PIFO 虚拟化** 是一种极具前景的解决方案，其核心思想是将一个物理 PIFO 队列虚拟化，用以服务多个逻辑 PIFO 节点。物理 PIFO 的实现可以划分为两个部分：**计算（computation）** 与 **存储（storage）**。在以往的工作中，这两部分是一一绑定、紧密耦合的。我们打破这种关系，将二者完全解耦，使多个逻辑 PIFO 节点的存储数据能够**时分复用**同一套计算资源。

这一机制类似于现代操作系统中在单个 CPU 上构建多进程抽象：通过频繁的上下文切换，每个进程都仿佛独占处理器。在 PIFO 虚拟化中，每个节点的数据看起来就像由一个独立的调度器进行排序。通过为虚拟化的 PIFO 队列引入一个类似**虚拟机监控器（hypervisor）**的组件，我们可以灵活调整支持的逻辑 PIFO 节点数量，并将其逻辑连接成不同的结构，以实现多种层次化调度算法，从而支持可编程层次化调度。此外，无论逻辑 PIFO 节点数量多少，所有计算资源都可以被充分利用。

除了支持可编程层次化调度外，PIFO 虚拟化在许多需要多个 PIFO 队列的场景中同样适用。例如，在一台 400 Gbps 的网络设备中，PIFO 虚拟化可以用于支持 10 个 40 Gbps 的虚拟端口，使每个端口看起来都拥有独立的流量调度器，从而显著降低硬件成本。

基于 PIFO 虚拟化的思想，我们提出了 **vPIFO 系统**。逻辑 PIFO 节点被称为 **PIFO 实例（PIFO instance）**。Packet In/Out 事件会根据 PIFO 树触发对应 PIFO 实例的 Push/Pop 操作（P/P 操作）。一个虚拟化的物理 PIFO 队列通过处理这些 P/P 操作，为多个 PIFO 实例完成分组排序。

## vPIFO Overview

为了在高速 MTDC 中支持可编程层次化调度，vPIFO 系统需要实现以下目标：

**目标 0：虚拟化支持**  
系统应当原生支持虚拟化，能够同时服务多个 PIFO 实例，并在实例之间实现有效的分组隔离。

**目标 1：高性能**  
系统必须具备线速调度能力，这要求满足以下子目标：

- **目标 1.1：高频率**——系统应以高时钟频率运行，在单位时间内处理大量 P/P 操作；
    
- **目标 1.2：低实例切换开销**——在处理不同 PIFO 实例的 P/P 操作时，上下文切换成本应尽可能低；
    
- **目标 1.3：高效的计算资源利用**——物理计算资源应当被高效分配给各个 PIFO 实例。  
    上述三点共同保证系统在实现可编程层次化调度时具备高性能。
    

**目标 2：大规模能力**  
系统应能够在存储资源受限的条件下，对大量流量执行复杂的层次化调度。这要求系统能够灵活配置 PIFO 实例数量及每个实例支持的流数量：

- **目标 2.1：支持大量 PIFO 实例**，以适配灵活多样的 PIFO 树结构；
    
- **目标 2.2：支持大量流**，以应对 MTDC 中庞大的流规模 [32]。
    

**目标 3：易用性**  
系统应为网络运营者和租户提供友好的使用体验，使其能够轻松描述调度需求，而无需理解复杂的硬件逻辑、进行繁琐的编程或解析层次化分组处理过程。这要求将高层调度需求与底层调度器实现解耦，并配备完善的自动化工具，以在硬件中实现层次化调度。

为实现上述目标，我们提出了一套软硬件协同的整体解决方案。vPIFO 系统由三个核心组件组成：  
1）**调度描述语言（Scheduling Description Language，SDL）及其编译器**，为用户提供描述调度需求的高层接口；  
2）**PIFO Engine**，作为支持虚拟化的物理 PIFO 调度器；  
3）**PIFO Visor**，作为硬件化的“调度器虚拟机监控器”，以周期为单位高效分配 PIFO Engine 的虚拟化计算资源。

图 3 展示了 vPIFO 系统的双阶段工作流程。第一阶段为初始化阶段（虚线表示）。在该阶段，租户和网络运营者使用 SDL 描述调度需求，SDL 编译器解析其中的 PIFO 实例及其调度关系，并生成**秩计算算法**、**操作生成规则**以及**数据放置方案**。秩计算算法以 SDL 中间表示（IR）的形式给出，可由不同平台的后端编译器进一步编译，通过 Packet Transaction 实现秩计算；操作生成规则和数据放置方案作为系统运行时的配置，其中前者用于构建**操作生成表**（§5.2.2），使 PIFO Visor 能够将 Packet In/Out 事件转换为 PIFO 实例的 P/P 操作；后者用于创建 **PIFO 实例地址表**（§6），为各个 PIFO 实例在 PIFO Engine 中分配存储资源。

第二阶段为运行阶段（图 3 中的实线）。在该阶段，PIFO Visor 接收携带秩值的分组，将 Packet In/Out 事件转换为 P/P 操作并存入操作队列；在每个时钟周期，PIFO Visor 通过操作分发器将 P/P 操作派发至 PIFO Engine 进行处理。

vPIFO 系统为 MTDC 提供了一种完整的调度解决方案。后续章节将详细介绍 SDL 的设计（§4）、PIFO Engine（§5）以及 PIFO Visor（§6）。在全文中，我们将持续强调设计目标，阐述设计取舍背后的考量及其带来的收益。