## Chapter 17: Make the Switch!
### Keeping House
目前switch的data path由硬件构建，但是仍需要嵌入式的微处理器支持数据路径以外的一切功能。
Housekeeping processor subsystem的组成如下：
- CPU
- Code/configuration Store（ROM）
	- 存储配置代码
- RAM
	- frame缓存
- local serial port
	- 初始化设备管理、系统debug、代码修改下载等
- 在交换机的端口与housekeeping subsystem之间也必须存在某种方式，用于在接收和发送方向上移动数据
#### Housekeeping Functions
- 生成树协议
- 链路聚合控制协议
- Marker协议
- 已启用的GARP应用
- 网络管理
- 内部诊断与维护
## Switch Data Receive Path Functions
![[25-12-10-1.png]]
### Port Interfaces（Receive）
Media Access Control（MAC）和Physical Layer receiver（PHY）构成了传统的网络接口功能。
- PHY receiver
	- 从传输介质获取电信号或光信号，根据所使用的具体技术将其解码成比特、半字节或字节流。
	- 不同的介质（例如双绞线或光纤）和不同的技术（例如不同的数据速率和 LAN 类型）通常需要不同的 PHY 实现。
- MAC
	- 完成必要的成帧和有效性检查，以便从PHY提供的数据流中重构出格式正确的帧。
	- 接收端 MAC 会在从 PHY 接收数据的同时实时计算帧校验序列（FCS），如果检测到无效的 FCS，MAC 就会丢弃该帧。除了可能为了管理目的而递增一个错误计数器之外，交换机没有必要继续处理任何接收错误的帧。
	- 接收 MAC 通常会维护大多数实时的基于端口的统计计数器。
	- 接收 MAC 必须将接收到的帧存储在本地缓冲区中。若为共享缓存架构，则可能可以仅将帧存储一次到一个公共缓冲池中，从而避免在输入和输出端口之间执行任何缓冲区复制操作。在这种架构下，交换机只需要维护**未处理缓冲区指针列表**，并通过调整各端口的队列数据结构即可，而不必真正将帧数据从端口复制到另一个端口。
从历史上看，PHY 和 MAC 设备是作为单独的封装组件构建的，每个集成电路（IC）只实现一个单元。现代硅工艺能力允许水平和/或垂直集成，也就是说：
- PHY 设备可以以四端口、六端口、八端口甚至更高密度的形式提供，适用于以太网或其他技术。
- MAC 设备同样可提供多实例的封装形式。
- MAC 和 PHY 也可以作为半导体内核，集成到同时执行交换功能的设备中，而不仅仅提供 MAC/PHY 接口。
### Receive Flow Control
在接口的接收侧，流控模块会检查进入的流量，以查找 PAUSE 帧（如第 8 章所述）。当接收到一个格式正确的 PAUSE 命令时，**该模块会根据帧中的 pause time 值更新暂停计时器，并向发送侧的流控模块提供相应的控制信号**。例如：
- 如果接收到一个 pause time 非零的 PAUSE 帧，则必须**通知接口的发送侧停止从 MAC 客户端（例如链路聚合分发器）接收待发送的帧**，直到暂停计时器到期。
- 如果接收到一个 pause time 为零的 PAUSE 帧，则发送侧可以重新启用，继续接收待发送的帧。
对 PAUSE 帧进行解析、解释并根据其执行操作，这些功能通常都在专用硬件中实现，原因如下：
- 相关操作简单、明确且范围有限。
- 在接收到 PAUSE 帧与停止发送帧之间的时间受到非常严格的限制：
    - 对于 10 Mb/s 或 100 Mb/s 的接口，最多 512 bit times；
    - 对于 1000 Mb/s 的接口，最多 1024 bit times。
当然，由于链路流量控制仅定义于以太网，对于 Token Ring 或 FDDI 接口则不存在接收流控模块。在这些情况下，所有的帧都会直接从接收 MAC 传递给链路聚合收集器或分类引擎。
### Link Aggregation Collector
如果多个物理接口聚合成一个逻辑接口，本模块负责实现帧收集功能（Collector）。Collector 从每个底层物理接口收集接收到的帧，并向分类引擎（Classification Engine）呈现一条合并后的单一帧流。Collector 不需要采取任何额外措施来确保从不同物理接口接收的帧之间的顺序正确性。链路的发送侧由 Distributor 负责确保：每个需要保持帧顺序的“会话”（conversation）都会被映射到聚合组中的某一个特定物理链路上。
Collector 还包含 Marker 协议中“响应者（Responder）”部分的实现；此功能可以通过有限状态机实现，也可以在常规微处理器上以软件方式实现。根据具体交换机的设计，通用 housekeeping 处理器也可能被用于生成 Marker Response，只是必须确保响应足够快速。通常，聚合链路另一端的 Distributor 会等待 Marker Response，然后才会把一个或多个会话从某条物理链路切换到另一条物理链路（见 17.4.3）。如果 Marker Response 延迟，会导致这些会话的通信被阻塞。
通常，链路聚合控制协议（LACP）会由 housekeeping 处理器实现，因为它对时间敏感性远低于 Marker 协议。LACP 帧始终使用处于“链路约束协议（link-constrained protocols）”保留地址范围内的多播目的地址；分类引擎会识别这些帧并标记为应本地接收（sink）到 housekeeping 处理器，如本文稍后将讨论的那样。
### Classification Engine
### VLAN Filters
### Lookup Engine
## Switch Fabrics
switch fabrics是发送端和接受端中间的部分，是交换机的核心。目前局域网中最常见的三种switch fabric架构为：shared memory、shared bus、crosspoint matrix
### Shared Memory
共享内存式架构**在设计和实现两方面都具有最低的复杂度**，并且在适用的产品中通常也是成本最低的解决方案。因此，它是局域网交换机中最常用的交换结构设计方式。
共享内存交换结构的主要限制在于其**内存带宽**：数据写入和读出共享内存的速度，会对交换机可支持的端口数量以及这些端口的数据速率形成上限。
![[25-12-10-2.png]]
#### Shared Memory Fabric Operation
共享内存交换结构（shared memory fabric）使用**单一的公共内存**作为端口之间帧交换的机制，如图 17-9 所示。通过输入端口接收数据路径到达的帧会被存放在共享内存中，然后根据需要被分配并导向相应输出端口的发送数据路径。
理想情况下，帧可以直接从接收端口接口写入共享内存。但在实际中，接收数据路径通常需要在每个端口上配备一定数量的本地内存，用于在接收和分类过程中临时存储帧。不过，系统中绝大部分的内存仍然是位于公共缓冲池中的共享内存。
最初，共享内存中的所有帧缓冲区都是空闲的，它们起始状态位于“空闲缓冲池”中。随着帧从各输入端口到达，这些空闲缓冲区会被分配给新帧。根据查找引擎（Lookup Engine）的结果，存放在共享内存中的帧会被链接到相应输出端口的输出队列中，这些输出队列代表了要在发送数据路径中处理的帧流（见 17.4 节）。丢弃帧的操作通过将其缓冲区归还到空闲池来完成。
使用共享内存结构的优点包括：
- **使用单一的大内存而不是每个端口独立的缓冲内存，可以降低整体内存成本（以及内存控制逻辑的成本）。**
- **从一个端口转发帧到另一个端口变得很简单**.如图 17-9 所示，每个端口的输出（发送）队列只是一个指向存放在共享内存中帧的缓冲指针的链表。将一个帧从输入端口转发到输出端口，只需将该帧缓冲区的指针链接到目标输出端口的队列即可。
- **查找引擎（Lookup Engine）可以使用共享的单一查找单元，也可以分布式部署（每个端口独立查找），取决于查找性能的需求（见 17.2.6 节）**.一些其他交换结构（例如交叉矩阵，见 17.3.3 节）很难使用单一的公共查找引擎。
- **查找操作可以在帧存入共享内存之前完成，也可以在帧存储后与存储操作并行进行**.也就是说，一个帧可以在交换机尚未确定如何处理它时就“先存进去”；一旦查找引擎完成工作，其缓冲指针就可以被链接到正确的输出队列中。
#### Multicasting in a Shared Memory Architecture
![[25-12-11-1.png]]
共享内存结构（shared memory fabric）使得能够将帧从一个输入端口转发到多个输出端口变得更加容易。由于所有端口都可以访问这块公共内存，因此不需要进行多次数据拷贝，也不需要为多播流量提供特殊的数据路径。每个帧缓冲区都可以配备一个“发送端口映射（Transmit Port Map）”，如图 17-10 所示。
查找引擎（Lookup Engine）决定该帧应被转发到哪些输出端口。**如果一帧需要发送到多个输出端口（例如，它是多播帧，或是未知单播帧），则会将发送端口映射中相应的目标端口位置比特设置为 1。**
当某个输出端口的发送数据路径不再需要这帧在共享内存中的副本时（例如，该端口已经完成了帧的发送，或者已经将帧复制到本地端口缓冲区），**它会清除发送端口映射中对应自己端口的比特，并检查是否仍有比特为 1。**
一旦发送端口映射中的所有比特都被清除，表示所有端口都已处理完该帧，该帧缓冲区就可以被归还到空闲池（free pool）。
#### Buffer Organization
##### Contiguous Buffers
当缓冲区是连续（contiguous）时，内存接口和控制逻辑的设计要简单得多。一个连续的缓冲区可以通过单一的指针标识，这个指针代表帧起始位置。用于在共享内存中搬移帧数据的 DMA（Direct Memory Access）引擎也可以仅依赖这个单一的缓冲区指针工作，无需将帧分片分散到共享内存中不同的区域，也不需要从不同区域收集数据。
**当帧缓冲区既是连续的又是固定长度时，复杂度最低**.在这种方案中，整个共享内存可以预先划分成独立、编号明确的帧缓冲区。这样就完全不需要复杂的动态内存分配逻辑。然而，每个固定长度的缓冲区都必须足够大，以容纳最大长度的帧；当接收的帧小于最大长度时，内存利用率会降低。因此，内存成本与内存接口逻辑的成本之间存在明显的权衡。
以太网交换机常用的一种方法，是分配固定长度、连续的 2,048（2K）字节缓冲区。一个 2KB 的缓冲区不仅能充分存下最大长度的以太网帧（1,522 字节，包括 VLAN 标签），还可以容纳内部交换头、发送端口映射（Transmit Port Map）以及任何与实现相关的内存控制信息。实际上，其中 526 字节留给内部开销的空间远远超过一般需求。然而，如果采用一个二进制的“整齐数值”作为缓冲区长度，内存接口逻辑可以将缓冲区编号作为每个内存指针的高位，如图 17-11 所示。这进一步简化了硬件逻辑，但代价是内存利用率的降低。
![[25-12-11-2.png]]
虽然可以设计使用可变长度连续缓冲区的共享内存交换结构，但这通常会显著增加内存接口的复杂度，并带来严重的内存碎片化。
##### Discontiguous Buffers
![[25-12-11-3.png]]与将一个帧存放在一块连续内存中不同，使用非连续（discontiguous）缓冲区时，一个帧可以由分散在内存各处的一系列帧段组成。**因此，帧的起始位置仍然由一个缓冲区指针标识，但该指针只对应帧的第一个段。整个帧需要通过遍历一个缓冲区指针的链表来引用，如图 17-12 所示。**
这些内存指针的链表**可以存放在输出队列的数据结构中（如图 17-12 所示），也可以存放在帧数据缓冲段自身内部，即在每个数据缓冲段的末尾包含下一个内存指针或一个表示链表结束的标志**。每个帧段可以具有可变长度。通常，硬件支持一个最小的段长度，并且每个段的总长度是该最小长度的整数倍。
与任何固定长度缓冲方案相比，非连续、可变长度缓冲区的方法能够更高效地利用内存。在最坏情况下，分配后未被使用的内存最多为 “一个最小长度缓冲区减去 1 字节”，而不是“一个最大帧的大小”。采用链表缓冲区方案，可以支持比固定长度缓冲方式更大的帧，同时不牺牲内存效率。
Token Ring 和 FDDI LAN 的帧长度比以太网更大；虽然在处理 1,522 字节以太网帧时，使用固定长度缓冲区带来的内存利用率下降通常可以接受，但对于一些 Token Ring 系统使用的 18 KB 帧而言，为了提高内存利用率，采用更复杂的内存接口可能是合理的。
#### Memory Bandwidth Limitations
简单来说，如果一个系统能够使用共享内存交换结构（shared memory switch fabric），那么它大概率就应该使用共享内存结构。共享内存架构通常是交换机设计中最简单、成本最低的解决方案。既然如此，为什么会有交换机设计者不选择共享内存方式呢？
原因在于：**所有帧都必须通过同一块共享内存**，因此内存接口的速度最终会限制交换机的总容量。
举例来说，假设内存系统具有：
- **32-bit 数据通路**
- **10 ns 同步静态 RAM (SSRAM)**
- **100 MHz 时钟**
每个帧至少必须经过内存接口两次：
1. 一次由输入端口写入共享内存 
2. 一次由输出端口从共享内存读出
这实际上**将可用的内存带宽减半**。
在最佳情况（流式访问：即只有**连续读写才能达到 10 ns 访问延迟，而随机访问无法达到**），交换机的总容量受限为：
$$
32\ \text{bits} \times 100\ \text{MHz} \div 2 = 1.6\ \text{Gb/s}  
$$
>为什么随机访问无法达到连续读写的性能？
>连续访问只进行一次地址译码，后续访问使用内部地址计数器；而随机访问每次都需要重新译码

但在实际系统中，不可能完全享受同步 RAM 的流式访问带来的加速；其真实性能提升取决于读写帧数据的消息系统设计。通常，可用的内存带宽只有理论最大值的 **50%–70%**。
此外，为了避免无界的排队延迟，**有效内存带宽至少需要超额预留（undersubscribe）30%**。
这意味着，可用的无阻塞交换容量大约只有原始带宽的 **35%–50%**，在上述例子中约为：
**550–800 Mb/s**
假设所有端口都工作在全双工模式，这个容量可以支持：
- 一个 **10 Mb/s、端口数量为 24–48** 的交换机    
- 或一个 **48 口 10 Mb/s + 两个 100 Mb/s 上行链路** 的交换机
但对于 **多于 8 个 100 Mb/s 端口** 的情况，该设计无法提供无阻塞性能。
因此，这个设计示例只适用于桌面型或小型工作组交换机。
#### Increasing the Memory Bandwidth
为了使共享内存交换结构能够用于更高性能的交换机，可以采取多种方法来增加内存带宽：
1. **使用最快的内存产品**  
    显然，更快的内存可以让系统设计者在更高的时钟频率下工作，而不会引入等待周期（wait states）。然而，由于当前半导体技术的物理限制，内存速度总是有上限。此外，最快的内存产品价格也最高，这会抵消共享内存架构的一些成本优势。
2. **使用更宽的内存数据通路**  
    即使内存自身的时钟受限，也可以通过加宽数据通路来提高总内存带宽。单个时钟周期传输更多位数据，内存带宽就会与内存接口宽度成正比。例如，将数据通路从 32 位改为 64 位即可将内存带宽翻倍，从而支持一个 12–16 端口的非阻塞 100 Mb/s 工作组交换机。
标准内存产品通常设计用于 16、32 或 64 位宽的数据通路。此外，内存技术的发展通常偏向于实现更高的存储密度，即在单个芯片上集成更多内存。以目前的 SSRAM 芯片为例，每位宽度大约对应 1 Mbit 深度。因此，一个 64 位宽的内存阵列通常至少需要 8 Mbyte（64 Mbit）的总容量。如果需要更高的内存带宽，可以使用 128 或 256 位宽的数据通路，但这会导致总内存容量翻倍或四倍（16 或 32 Mbyte），因为必须使用标准内存芯片。
在很多情况下，这样做虽然提高了内存带宽，但总内存容量远超过交换机实际需求。由于传统内存产品并未针对“宽而浅”的内存阵列进行优化，你往往不得不为不需要的额外内存支付费用，仅仅为了获得更快的数据通路。
3. **使用非传统内存**  
    除了通过加宽传统内存数据通路的“暴力”方法外，还有一些替代内存设计可提供更高接口带宽，包括：
	- **同步图形内存（SGRAM）**：类似于局域网交换机，图形显示子系统通常需要比传统计算机内存更宽、更浅的内存配置。一些半导体厂商提供了具有这种特性的特殊 SGRAM。它偶尔被用于局域网交换机，但由于销量低、市场有限，价格比传统 RAM 高。
	- **Rambus 动态内存（RDRAM）**：设计用于高性能 PC、工作站和服务器，RDRAM 支持 600 MHz 及更高的接口时钟频率。这类高性能内存价格也较高，但可以将共享内存交换结构应用到更高端的交换机产品中。
	- **使用嵌入式内存**：即使你愿意为实现 4–6 Gb/s 带宽的交换机购买 32 Mbyte、256 位宽的 SSRAM 阵列，连接这些外部内存所需的引脚数量也是一个问题。高性能交换机所用的定制 ASIC 芯片成本中，很大一部分与封装要求相关。如果仅连接内存就需要超过 256 个引脚（包括仲裁、控制、电源信号以及数据通路），将显著增加交换机成本。
越来越有吸引力的方案是将内存嵌入到交换机 ASIC 内部。如果内存是片内的，而非外部芯片，就无需引脚（或封装焊盘）。此外，片内逻辑时钟可以比外部内存更快，因为电感更低、电流驱动更小。嵌入式内存的数据通路宽度可达到 512 位或更高，而且几乎不增加成本。
### Crosspoint Matrix
![[25-12-11-4.png]]
如图 17-14 所示，**交叉点矩阵（crosspoint matrix）交换结构**在一个帧（或帧的一部分）传输期间，为输入端口和输出端口之间创建一个临时连接。实际上，该结构包含一个电子开关矩阵，可以在任意输入与任意输出端口之间建立连接。
交换结构的控制逻辑会在**每一帧**的基础上，用**纳秒级的时间将某个输入端口连接至相应的输出端口，然后在准备下一次帧传输前断开这些端口的连接**。
交叉点矩阵交换结构通常也被称为 **crossbar（交叉开关）**；两个术语是同义的。
由于每个输入端口可以同时被连接到某个输出端口，因此交叉点矩阵能够提供的总数据传输能力远高于共享总线结构，且其带宽**会随着端口数量的增加而增长**。
#### Multicasting in a Crosspoint Matrix Fabric
与共享内存或共享总线架构不同，**交叉点矩阵（crosspoint matrix）本身不具备让一个输入端口同时向多个输出端口传输帧的天然机制**。在 crosspoint 结构中，通常有两种方法处理多播（multicast）流量：
**1. 使用多次单播传输（Multiple Unicast Transfers）**
简单来说，当一个帧需要被发送到多个输出端口时，输入端口可以依次对每个目标输出端口执行一次独立的帧传输。
与共享内存架构类似，输入端口会维护一个 **Transmit Port Map**，表示该帧需要被传输到的所有目标输出端口集合。随着交换结构的仲裁和帧传输进行，每成功传输到一个输出端口，就在该列表中将该端口标记为已完成。当 Transmit Port Map 中所有位都变为 0 后，输入端口即可释放该帧缓冲。
**优点：**
- 简化 crosspoint 矩阵交换结构的设计。不需要为多播流量特别设计额外机制。
**缺点：**
- 输入端口的逻辑变得更复杂，需要跟踪多播帧的传输进度。
虽然多次传输同一帧看上去会浪费交换结构的容量，但实际上，一个多播帧最终仍然必须占用所有目标输出端口的带宽。  
如果交换结构被设计为可支持所有端口带宽总和，那么即便多播帧以多次单播方式发送，系统仍然是 **non-blocking（无阻塞）** 的。
不过，任何用于设计时的**额外带宽余量（over provisioning）**，都会因为多播复制而减少。系统依然能处理全部流量，但**队列时延（交换延迟）会增加**，尤其当多播流量占比显著时。
**2. 提供专用的多播数据路径（Dedicated Multicast Data Path）**
另一种方法是在交换机中提供一个或多个专用于多播流量的逻辑输出端口。
该**多播端口本质上是一个共享总线（shared bus）**，它被连接到所有输出端口，并与每个端口的单播专用线路并行，如图 17-15 所示。
![[25-12-11-5.png]]
当以下情况成立时，这种方法是合理的：
- 多播流量占总负载比例较小 
- 交换矩阵本身是阻塞式的或可用余量不大
通过将多播流量从交换矩阵的独立端口输出线路上分离出来，就避免了前面提到的帧复制问题，从而可以把大部分交换能力保留给单播流量（假设单播是主要流量）。
**这种方法的缺点包括：**
1. **每个输出端口必须能够同时接收两路数据：**
    - 来自交换矩阵的专用单播输出
    - 来自共享多播端口的多播输出
2. **共享多播总线本身可能成为瓶颈**
    如果多播流量占总负载比例较大，共享多播总线的容量可能无法满足需求。
> 如果 crossbar 是 **blocking** 或 **没有带宽余量**，则每次连接可能都要等待仲裁，多播帧的复制会占掉大量 fabric 带宽，尤其是多播流量一多，会让 **整个交换机的单播流量受到影响**
#### Crosspoint Matrix Implementation
实现交叉点矩阵（crosspoint matrix）的两种常见方法如下：
**1. 物理矩阵（physical matrix）**
这种矩阵可以通过以下方式构建：  
在一个平面上平行铺设导电材料条带（对应输入端口），在另一个平面上以直角方向铺设导电条带（对应输出端口）。  
每一条输入端口的导线都通过一个晶体管开关电路连接到每一个输出端口的导线；  
交换控制逻辑根据输入端口控制器的请求以及仲裁器的仲裁结果（参考 17.3.3.5 节），决定哪些晶体管应该闭合或断开。  
也就是说，图 17-14 所示的就是这种物理交叉点矩阵的结构。
根据交换矩阵的数据速率，往往需要为每个端口使用多条导线轨迹（输入端口和输出端口都是如此）。  
这样，每个时钟周期可以传输多个比特数据，从而降低时钟频率。但其代价是交换芯片的端口引脚数量会增加。
**2. 逻辑矩阵（logical matrix）**
这种矩阵通过逻辑组合实现：  
每个输出端口的信号由输入端口信号经过某种逻辑函数生成，而这个逻辑关系由**交换矩阵仲裁器**控制。  
换言之，交叉点矩阵可以完全用标准数字逻辑模块实现。
与物理矩阵类似，也可以通过为每个输入和输出定义多条并行信号来降低时钟速度，但这同样会增加接口的引脚数量。
**两种实现方式的比较**
- **物理矩阵**
    - 优点：规模更小、复杂度更低，能实现更低成本的交换矩阵。   
    - 缺点：电路往往必须定制布局，不适合使用常规的 IC 综合设计工具在硅片上生成。  
- **逻辑矩阵**
    - 优点：使用可综合逻辑，因此更容易在不同的半导体工艺之间迁移，也不需要非常专门的芯片布局设计技能。
    - 缺点：可能更大、更复杂。
根据时钟频率和端口宽度，逻辑矩阵可以实现非常大的交换能力。例如，一个 **16 端口、每端口 16bit 宽、100MHz 时钟** 的矩阵，其有效数据交换能力可达：
**16 ports × 16 bits × 100 MHz = 25.6 Gb/s**
这样的交换矩阵可以支持大量的 **100Mb/s 端口** 或相当数量的 **1GbE 端口**。
